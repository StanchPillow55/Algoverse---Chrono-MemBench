{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model Training - Google Colab\n",
    "\n",
    "This notebook demonstrates how to train Gemma-2B, Llama-3-8B, and LLaVA-1.6 models using your custom datasets.\n",
    "\n",
    "## Setup Instructions:\n",
    "1. Upload your datasets to Google Drive\n",
    "2. Choose your model configuration\n",
    "3. Run the training cells\n",
    "4. Monitor training progress\n",
    "5. Download your trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi\n",
    "\n",
    "# Install required packages\n",
    "!pip install -q transformers peft datasets torch accelerate tensorboard PyYAML\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone your repository or upload files\n",
    "import os\n",
    "os.chdir('/content')\n",
    "\n",
    "# Option 1: Clone from GitHub (recommended)\n",
    "# !git clone https://github.com/yourusername/chrono-membench.git\n",
    "# os.chdir('/content/chrono-membench')\n",
    "\n",
    "# Option 2: Copy from Google Drive\n",
    "!cp -r /content/drive/MyDrive/chrono-membench /content/\n",
    "os.chdir('/content/chrono-membench')\n",
    "\n",
    "# Verify structure\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using DVC, pull the datasets\n",
    "# !dvc pull\n",
    "\n",
    "# Or copy datasets from Google Drive\n",
    "!mkdir -p data/raw\n",
    "!cp /content/drive/MyDrive/chrono-membench/data/raw/*.jsonl data/raw/\n",
    "\n",
    "# Verify datasets\n",
    "!ls -lah data/raw/*.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose your model and configuration\n",
    "MODEL_TYPE = \"gemma-2b\"  # Options: \"gemma-2b\", \"llama-3-8b\", \"llava-1.6-7b\"\n",
    "MODEL_SOURCE = \"huggingface\"  # Options: \"local\", \"huggingface\"\n",
    "CONFIG_FILE = f\"configs/{MODEL_TYPE}.yaml\"\n",
    "\n",
    "print(f\"Training {MODEL_TYPE} using {MODEL_SOURCE} source\")\n",
    "print(f\"Configuration file: {CONFIG_FILE}\")\n",
    "\n",
    "# View configuration\n",
    "!cat {CONFIG_FILE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. HuggingFace Authentication (if using HuggingFace models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to HuggingFace if using HuggingFace models\n",
    "if MODEL_SOURCE == \"huggingface\":\n",
    "    from huggingface_hub import login\n",
    "    \n",
    "    # Enter your HuggingFace token\n",
    "    token = input(\"Enter your HuggingFace token: \")\n",
    "    login(token)\n",
    "    \n",
    "    print(\"HuggingFace authentication successful!\")\n",
    "else:\n",
    "    print(\"Using local models - no authentication needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Configuration Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update configuration for Colab environment\n",
    "import yaml\n",
    "\n",
    "with open(CONFIG_FILE, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Update for Colab\n",
    "config['model']['source'] = MODEL_SOURCE\n",
    "config['environment']['platform'] = 'colab'\n",
    "config['environment']['mixed_precision'] = 'fp16'  # Good for Colab\n",
    "config['training']['batch_size'] = 2  # Conservative for Colab\n",
    "config['training']['gradient_accumulation_steps'] = 8\n",
    "config['colab']['mount_drive'] = True\n",
    "config['colab']['install_requirements'] = True\n",
    "\n",
    "# Save updated config\n",
    "colab_config_file = f\"configs/{MODEL_TYPE}_colab.yaml\"\n",
    "with open(colab_config_file, 'w') as f:\n",
    "    yaml.dump(config, f, default_flow_style=False)\n",
    "\n",
    "print(f\"Updated configuration saved to {colab_config_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Set up Python path\n",
    "sys.path.append('/content/chrono-membench/src')\n",
    "\n",
    "# Run training\n",
    "cmd = [\n",
    "    'python', '/content/chrono-membench/src/chrono/train.py',\n",
    "    '--config', colab_config_file,\n",
    "    '--output_dir', f'/content/outputs/{MODEL_TYPE}',\n",
    "    '--base_path', '/content/chrono-membench'\n",
    "]\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(f\"Command: {' '.join(cmd)}\")\n",
    "\n",
    "# Run training\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "\n",
    "print(\"STDOUT:\")\n",
    "print(result.stdout)\n",
    "\n",
    "if result.stderr:\n",
    "    print(\"\\nSTDERR:\")\n",
    "    print(result.stderr)\n",
    "\n",
    "print(f\"\\nTraining completed with exit code: {result.returncode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Monitor Training (Alternative - Interactive Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Run training interactively to see progress\n",
    "!cd /content/chrono-membench && python src/chrono/train.py \\\n",
    "    --config {colab_config_file} \\\n",
    "    --output_dir /content/outputs/{MODEL_TYPE} \\\n",
    "    --base_path /content/chrono-membench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. View Training Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View training logs\n",
    "!ls -la /content/outputs/{MODEL_TYPE}/\n",
    "\n",
    "# Check if TensorBoard logs exist\n",
    "!ls -la /content/outputs/{MODEL_TYPE}/logs/\n",
    "\n",
    "# Load TensorBoard (optional)\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /content/outputs/{MODEL_TYPE}/logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Load the trained model\n",
    "model_path = f\"/content/outputs/{MODEL_TYPE}\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Test generation\n",
    "def generate_text(prompt, max_length=100):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=True\n",
    "        )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"The future of artificial intelligence is\",\n",
    "    \"In mathematics, the concept of infinity\",\n",
    "    \"Climate change is a global challenge that\",\n",
    "    \"Question: What is 2+2? Answer:\"\n",
    "]\n",
    "\n",
    "print(\"Testing trained model:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    generated = generate_text(prompt)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generated: {generated}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Model to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model to Google Drive\n",
    "import shutil\n",
    "\n",
    "# Create destination directory\n",
    "drive_model_path = f\"/content/drive/MyDrive/trained_models/{MODEL_TYPE}\"\n",
    "!mkdir -p {drive_model_path}\n",
    "\n",
    "# Copy model files\n",
    "source_path = f\"/content/outputs/{MODEL_TYPE}\"\n",
    "!cp -r {source_path}/* {drive_model_path}/\n",
    "\n",
    "print(f\"Model saved to Google Drive: {drive_model_path}\")\n",
    "\n",
    "# Also save training logs\n",
    "logs_path = f\"/content/drive/MyDrive/training_logs/{MODEL_TYPE}\"\n",
    "!mkdir -p {logs_path}\n",
    "!cp -r {source_path}/logs/* {logs_path}/\n",
    "\n",
    "print(f\"Training logs saved to: {logs_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up temporary files to free space\n",
    "!rm -rf /content/outputs\n",
    "!rm -rf /content/chrono-membench\n",
    "\n",
    "print(\"Cleanup completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
