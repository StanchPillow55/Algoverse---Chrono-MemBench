{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StanchPillow55/Algoverse---Chrono-MemBench/blob/main/notebooks/chrono_membench_colab_universal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# Chrono-MemBench Training on Google Colab\n",
        "\n",
        "This notebook provides a complete setup for running chrono-membench training with temporal dropout and Route-SAE on Google Colab.\n",
        "\n",
        "## Features\n",
        "- Temporal dropout regularization\n",
        "- Route-SAE (Sparse Autoencoder) integration\n",
        "- Feature alignment loss\n",
        "- WandB monitoring\n",
        "- Support for Gemma-2B and Llama-3-8B\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## 1. Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mount_drive"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Change to content directory\n",
        "import os\n",
        "os.chdir('/content')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clone_repo"
      },
      "outputs": [],
      "source": [
        "# Clone the repository\n",
        "!git clone https://github.com/StanchPillow55/Algoverse---Chrono-MemBench.git\n",
        "\n",
        "# Change to the repository directory\n",
        "os.chdir('/content/Algoverse---Chrono-MemBench')\n",
        "\n",
        "# Verify the structure\n",
        "!ls -la\n",
        "!ls -la configs/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_deps"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -r requirements_training.txt\n",
        "!pip install wandb accelerate\n",
        "\n",
        "# Install additional dependencies for Colab\n",
        "!pip install transformers datasets torch torchvision torchaudio\n",
        "!pip install peft bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "config_section"
      },
      "source": [
        "## 2. Configure Training Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "set_params"
      },
      "outputs": [],
      "source": [
        "# Set training parameters\n",
        "MODEL_TYPE = 'gemma-2b'  # or 'llama-3-8b'\n",
        "\n",
        "# Model source options:\n",
        "# 'huggingface' - Stream from HuggingFace Hub (recommended, ~5GB download)\n",
        "# 'local' - Use models uploaded to Google Drive (faster if you have them)\n",
        "MODEL_SOURCE = 'huggingface'  # Change to 'local' if you uploaded models to Drive\n",
        "\n",
        "# Local model paths (if using MODEL_SOURCE = 'local')\n",
        "LOCAL_MODEL_BASE = '/content/drive/MyDrive/chrono_models'\n",
        "\n",
        "OUTPUT_DIR = '/content/drive/MyDrive/chrono_outputs'  # Save to Google Drive\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Model: {MODEL_TYPE}\")\n",
        "print(f\"Source: {MODEL_SOURCE}\")\n",
        "if MODEL_SOURCE == 'local':\n",
        "    print(f\"Local models path: {LOCAL_MODEL_BASE}\")\n",
        "print(f\"Output: {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "update_config"
      },
      "outputs": [],
      "source": [
        "# Update configuration for Colab environment\n",
        "import yaml\n",
        "\n",
        "# Use the correct config file path (with underscore)\n",
        "CONFIG_FILE = f'configs/{MODEL_TYPE.replace(\"-\", \"_\")}.yaml'\n",
        "print(f\"Using config file: {CONFIG_FILE}\")\n",
        "\n",
        "# Check if config file exists\n",
        "if not os.path.exists(CONFIG_FILE):\n",
        "    print(f\"Config file not found: {CONFIG_FILE}\")\n",
        "    print(\"Available config files:\")\n",
        "    !ls -la configs/\n",
        "    # Use the chrono_membench config as fallback\n",
        "    CONFIG_FILE = 'configs/chrono_membench.yaml'\n",
        "    print(f\"Using fallback config: {CONFIG_FILE}\")\n",
        "\n",
        "with open(CONFIG_FILE, 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "# Ensure model section exists and add paths if missing (fix for KeyError)\n",
        "if 'model' not in config:\n",
        "    config['model'] = {}\n",
        "\n",
        "# Add paths section if missing\n",
        "if 'paths' not in config['model']:\n",
        "    config['model']['paths'] = {\n",
        "        'gemma-2b': {\n",
        "            'local': f'{LOCAL_MODEL_BASE}/gemma-2b',\n",
        "            'huggingface': 'google/gemma-2b'\n",
        "        },\n",
        "        'llama-3-8b': {\n",
        "            'local': f'{LOCAL_MODEL_BASE}/llama3-8b',\n",
        "            'huggingface': 'meta-llama/Meta-Llama-3-8B'\n",
        "        }\n",
        "    }\n",
        "\n",
        "# Update local paths to use Google Drive if using local models\n",
        "if MODEL_SOURCE == 'local':\n",
        "    for model_key in config['model']['paths']:\n",
        "        config['model']['paths'][model_key]['local'] = f'{LOCAL_MODEL_BASE}/{model_key.replace(\"-\", \"\")}'\n"
        "\n",
        "# Update for Colab\n",
        "config['model']['source'] = MODEL_SOURCE\n",
        "config['model']['type'] = MODEL_TYPE\n",
        "config['environment']['platform'] = 'colab'\n",
        "config['environment']['mixed_precision'] = 'fp16'  # Good for Colab\n",
        "config['training']['batch_size'] = 1  # Very conservative for Colab\n",
        "config['training']['gradient_accumulation_steps'] = 16\n",
        "config['training']['max_steps'] = 100  # Shorter for demo\n",
        "config['training']['eval_steps'] = 25\n",
        "config['training']['save_steps'] = 50\n",
        "\n",
        "# Ensure colab section exists\n",
        "if 'colab' not in config:\n",
        "    config['colab'] = {}\n",
        "config['colab']['mount_drive'] = True\n",
        "config['colab']['install_requirements'] = True\n"
        "\n",
        "# Save updated config\n",
        "colab_config_file = f\"configs/{MODEL_TYPE}_colab.yaml\"\n",
        "with open(colab_config_file, 'w') as f:\n",
        "    yaml.dump(config, f, default_flow_style=False)\n",
        "\n",
        "print(f\"Updated configuration saved to {colab_config_file}\")\n",
        "print(f\"Training will run for {config['training']['max_steps']} steps\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model_setup_section"
      },
      "source": [
        "## 3. Model Setup (Local vs HuggingFace)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "check_model_setup"
      },
      "outputs": [],
      "source": [
        "# Check model availability and setup\n",
        "\n",
        "if MODEL_SOURCE == 'local':\n",
        "    print(\"üîç Checking for local models in Google Drive...\")\n",
        "    \n",
        "    model_path = f\"{LOCAL_MODEL_BASE}/{MODEL_TYPE.replace('-', '')}\"\n",
        "    \n",
        "    if os.path.exists(model_path):\n",
        "        print(f\"‚úÖ Found local model at: {model_path}\")\n",
        "        \n",
        "        # Check model files\n",
        "        model_files = os.listdir(model_path)\n",
        "        required_files = ['config.json', 'tokenizer.json']\n",
        "        \n",
        "        missing_files = [f for f in required_files if f not in model_files]\n",
        "        \n",
        "        if missing_files:\n",
        "            print(f\"‚ö†Ô∏è  Missing files: {missing_files}\")\n",
        "            print(\"Model may be incomplete. Consider using 'huggingface' source.\")\n",
        "        else:\n",
        "            print(\"‚úÖ Model appears complete\")\n",
        "            \n",
        "        # Show model size\n",
        "        import subprocess\n",
        "        try:\n",
        "            result = subprocess.run(['du', '-sh', model_path], capture_output=True, text=True)\n",
        "            size = result.stdout.split()[0]\n",
        "            print(f\"üì¶ Model size: {size}\")\n",
        "        except:\n",
        "            print(\"üì¶ Could not determine model size\")\n",
        "    else:\n",
        "        print(f\"‚ùå Local model not found at: {model_path}\")\n",
        "        print(\"\\nüìã To use local models:\")\n",
        "        print(\"1. Upload your model folder to Google Drive\")\n",
        "        print(f\"2. Place it at: {LOCAL_MODEL_BASE}/\")\n",
        "        print(\"3. Folder structure should be:\")\n",
        "        print(f\"   {LOCAL_MODEL_BASE}/\")\n",
        "        print(f\"   ‚îú‚îÄ‚îÄ gemma2b/          # For Gemma-2B\")\n",
        "        print(f\"   ‚îÇ   ‚îú‚îÄ‚îÄ config.json\")\n",
        "        print(f\"   ‚îÇ   ‚îú‚îÄ‚îÄ tokenizer.json\")\n",
        "        print(f\"   ‚îÇ   ‚îî‚îÄ‚îÄ model.safetensors or pytorch_model.bin\")\n",
        "        print(f\"   ‚îî‚îÄ‚îÄ llama38b/         # For Llama-3-8B\")\n",
        "        print(\"\\nüîÑ Switching to HuggingFace source for now...\")\n",
        "        MODEL_SOURCE = 'huggingface'\n",
        "        \n",
        "elif MODEL_SOURCE == 'huggingface':\n",
        "    print(\"üåê Using HuggingFace Hub - models will be downloaded automatically\")\n",
        "    \n",
        "    model_sizes = {\n",
        "        'gemma-2b': '~5GB',\n",
        "        'llama-3-8b': '~16GB'\n",
        "    }\n",
        "    \n",
        "    print(f\"üì¶ Expected download size: {model_sizes.get(MODEL_TYPE, 'Unknown')}\")\n",
        "    print(\"‚è±Ô∏è  Download time: 2-5 minutes on Colab\")\n",
        "    print(\"üíæ Models will be cached in Colab's temporary storage\")\n",
        "\n",
        "print(f\"\\nüéØ Final setup: {MODEL_TYPE} from {MODEL_SOURCE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wandb_section"
      },
      "source": [
        "## 4. Setup WandB (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup_wandb"
      },
      "outputs": [],
      "source": [
        "# Setup WandB for monitoring (optional)\n",
        "import wandb\n",
        "\n",
        "# Login to wandb (you'll need to enter your API key)\n",
        "wandb.login()\n",
        "\n",
        "# Initialize wandb project\n",
        "wandb.init(project=\"chrono-membench-colab\", name=f\"{MODEL_TYPE}-demo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training_section"
      },
      "source": [
## 5. MacOS/Apple Silicon Setup

These cells will help you set up training using MPS on an Apple Silicon Mac.

### Check MPS Setup

You can check for Metal Performance Shaders (MPS) and Apple Silicon support.

```python
# Check if MPS is available
!python -c "import torch; print(f'MPS available: {torch.backends.mps.is_available()}')"

# Verify MacOS setup
!python scripts/train_macos.py --check_only

# Check Apple Silicon info
!system_profiler SPHardwareDataType | grep "Chip\|Memory"
```

### Alternative Training

If you want to run training specific to macOS, use this cell:

```python
# Run training on Apple Silicon
!python scripts/train_macos.py --config configs/chrono_membench.yaml
```

## 6. Run Training
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "check_gpu"
      },
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "else:\n",
        "    print(\"No GPU available - training will be very slow!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_training"
      },
      "outputs": [],
      "source": [
        "# Run chrono-membench training\n",
        "!python src/chrono_cli.py chrono-train \\\n",
        "    --config {colab_config_file} \\\n",
        "    --output_dir {OUTPUT_DIR} \\\n",
        "    --sae_enabled \\\n",
        "    --feature_alignment_enabled \\\n",
        "    --temporal_dropout_rate 0.2 \\\n",
        "    --wandb_project chrono-membench-colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alternative_section"
      },
      "source": [
        "## 5. Alternative: Quick Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quick_training"
      },
      "outputs": [],
      "source": [
        "# Alternative: Run quick training (if the above doesn't work)\n",
        "!python src/chrono_cli.py quick-chrono {MODEL_TYPE} --output_dir {OUTPUT_DIR}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "results_section"
      },
      "source": [
        "## 6. Analyze Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "analyze_results"
      },
      "outputs": [],
      "source": [
        "# Analyze training results\n",
        "!python src/chrono_cli.py analyze-results --results_dir {OUTPUT_DIR}\n",
        "\n",
        "# Check output directory\n",
        "!ls -la {OUTPUT_DIR}\n",
        "\n",
        "# Display evaluation report if it exists\n",
        "import json\n",
        "eval_report_path = f\"{OUTPUT_DIR}/evaluation_report.json\"\n",
        "if os.path.exists(eval_report_path):\n",
        "    with open(eval_report_path, 'r') as f:\n",
        "        report = json.load(f)\n",
        "    print(\"\\nEvaluation Report:\")\n",
        "    print(json.dumps(report, indent=2))\n",
        "else:\n",
        "    print(f\"Evaluation report not found at {eval_report_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "troubleshooting"
      },
      "source": [
        "## 7. Troubleshooting\n",
        "\n",
        "If you encounter issues:\n",
        "\n",
        "1. **Out of Memory**: Reduce batch size or use gradient checkpointing\n",
        "2. **Config File Not Found**: Check the config file path and ensure it exists\n",
        "3. **Model Loading Issues**: Verify HuggingFace access and model availability\n",
        "4. **Data Loading Issues**: Ensure data files are available and accessible\n",
        "\n",
        "For more detailed troubleshooting, refer to the `CHRONO_TRAINING_README.md` file in the repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "debug_info"
      },
      "outputs": [],
      "source": [
        "# Debug information\n",
        "print(\"Repository structure:\")\n",
        "!find . -name \"*.py\" -o -name \"*.yaml\" | head -20\n",
        "\n",
        "print(\"\\nConfig files:\")\n",
        "!ls -la configs/\n",
        "\n",
        "print(\"\\nData files:\")\n",
        "!ls -la data/raw/ || echo \"No data/raw directory found\"\n",
        "\n",
        "print(\"\\nPython environment:\")\n",
        "!python --version\n",
        "!pip list | grep -E \"torch|transformers|peft|wandb\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
