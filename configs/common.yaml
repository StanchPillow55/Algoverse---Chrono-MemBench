model_name_or_path: openai-community/gpt2
tokenizer_name: openai-community/gpt2
sequence_length: 1024
fp16: true
gradient_checkpointing: true
optim: adamw_torch_fused
learning_rate: 2.0e-4
lr_schedule_type: cosine
weight_decay: 0.10
warmup_tokens: 5e6
total_tokens: 4.8e8  # 12 Ã— 40 M
max_grad_norm: 1.0
logging_steps: 25
eval_steps: 500
save_strategy: steps
save_total_limit: 4
push_to_hub: true
hub_model_id: my-org/chatgpt-2-0
